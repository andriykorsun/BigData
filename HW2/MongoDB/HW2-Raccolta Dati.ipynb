{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2 - Raccolta dati (estrazione dei topic e preprocessing)\n",
    "\n",
    "## Installazione delle librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import time\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lettura del dataset (formato .csv)\n",
    "\n",
    "Leggiamo il dataset dei progetti della Federico II, effettuando un preprocessing su alcune colonne del dataset che vengono eliminate perch√® non usate ai fini dell'analisi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Dataset/Dataset_Projects_Unina.csv', header=0)\n",
    "df.drop(labels=['Abstract_translated','Keywords','Researchers','City_of_Research_organization','Program'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrazione dei topic dal titolo dei progetti\n",
    "\n",
    "Tramite le API di OpenAI si estrae un topic sintetico che caratterizza il progetto a partire dal suo titolo.\n",
    "<br><br>\n",
    "**NOTA:** Si inserisce tale topic come colonna del dataset dei progetti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"INSERIRE API-KEY\"\n",
    "\n",
    "topic = []\n",
    "for title in df['Title_translated']:\n",
    "    prompt = \"Given the title of the following research project: '{}' generate the hypothetic topic with a word limit of 20\".format(title)\n",
    "\n",
    "    if len(title) != 0:\n",
    "        #message = {\"role\": \"user\", \"content\": prompt}\n",
    "        completion = openai.Completion.create(model=\"text-davinci-003\", prompt=prompt, temperature=0, max_tokens=150)\n",
    "        topic.append(completion['choices'][0]['text'])\n",
    "    \n",
    "    print('Titolo: ' + title + ' Topic: ' + completion['choices'][0]['text'] + ' \\n')\n",
    "    time.sleep(5) # Sleep di 5 secondi per evitare errori di RateLimit imposti da OpenAI\n",
    "\n",
    "df['topic'] = topic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing dei topic\n",
    "\n",
    "Si effettua una fase di **lemmatizing**, in cui avviene il processo di normalizzazione linguistica che consiste nel ridurre una parola alla sua forma base (es. da plurale a singolare).\n",
    "<br><br>\n",
    "**NOTA:** Per effettuare tale operazione si usa la funzione _WordNetLemmatizer()_ della libreria **NLTK (\"Natural Language Toolkit\")**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in words])\n",
    "\n",
    "df['topic'] = df['topic'].apply(lemmatize_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvataggio del dataset preprocessato (formato .csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./Dataset/Dataset_Projects_Unina_with_Topic.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mongo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
